---
layout: post
title: 最近大家都在聊GC呢
date: 2018-11-19 23:30:26
categories : [技术]
tags: []
---

### 发现问题

最近有人说调用我们服务偶发性失败，问我是不是频繁上线，首先我们当时并没有上线，其次即使是在上线也不应该用户有感知啊，但是我从服务端来看却又没有任何问题，整个服务的负载也都挺健康的，后来仔细对比时间和监控发现该服务时不时的发生一次长达 1.x 秒的 G1 Young GC（这个时间我是在监控上看到的，不过后来得知我们这个监控的 Young GC 和 MIXED GC 是混在一起的，所以也可能是个 1.x 秒的 MIXED GC），而最近好像大家都在聊 GC，于是我也准备自己来研究下这个问题。

### 解决问题

为了解决问题，首先增加了一些打印 GC 日志的参数：

    -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1

具体的含义如下：

```-XX:+PrintGCDetails```  输出GC的详细日志
```-XX:+PrintGCTimeStamps```  输出GC的时间戳（以基准时间的形式）
```-XX:+PrintGCDateStamps```  输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）
```-XX:+PrintTenuringDistribution```  在每次新生代GC时，输出幸存区中对象的年龄分布
```-XX:+PrintGCApplicationStoppedTime```  输出全部的JVM停顿（STW）时间（不只是GC）
```-XX:+PrintSafepointStatistics``` ```-XX:PrintSafepointStatisticsCount=1```  记录STW发生的原因、线程情况、STW各个阶段的停顿时间
```-XX:+PrintHeapAtGC```  在进行GC的前后打印出堆的信息

打印出来的 GC 日志如下：<!--more-->

<img src="/2018/11/19/最近大家都在聊GC呢/52fe23f7-96b6-467f-b809-5906afff6dce.png" width="50%" height="50%" alt="" align=center />

日志打印出来之后发现在做 long gc 的时候是因为出现了 ```to-space exhausted```，但其实周围并没有大对象，而且此时 eden 区减少了 4.8G，但是整个 Heap 只减少了 2.4G，所以怀疑是 gc 的时候发现 to-apace 不够，所以把 young gen 的东西一股脑扔进了 old gen，然后开始调整各种 GC 参数准备解决这个问题。

把 GC 日志贴到 [GCeasy](http://gceasy.io/) 上面是这样反馈的：

> 14.77% of GC time (i.e 1 sec 692 ms) is caused by 'Evacuation Failure'. When there are no more free regions to promote to the old generation or to copy to the survivor space, and the heap cannot expand since it is already at its maximum, an evacuation failure occurs. For G1 GC, an evacuation failure is very expensive - a. For successfully copied objects, G1 needs to update the references and the regions have to be tenured. b. For unsuccessfully copied objects, G1 will self-forward them and tenure the regions in place.
> Solution: 
> 1. Evacuation failure might happen because of over tuning. So eliminate all the memory related properties and keep only min and max heap and a realistic pause time goal (i.e. Use only -Xms, -Xmx and a pause time goal -XX:MaxGCPauseMillis). Remove any additional heap sizing such as -Xmn, -XX:NewSize, -XX:MaxNewSize, -XX:SurvivorRatio, etc.
> 2. If the problem still persists then increase JVM heap size (i.e. -Xmx)
> 3. If you can't increase the heap size and if you notice that the marking cycle is not starting early enough to reclaim the old generation then reduce -XX:InitiatingHeapOccupancyPercent. The default value is 45%. Reducing the value will start the marking cycle earlier. On the other hand, if the marking cycle is starting early and not reclaiming, increase the -XX:InitiatingHeapOccupancyPercent threshold above the default value.
> 4. If concurrent marking cycles are starting on time, but takes long time to finish then increase the number of concurrent marking thread count using the property: '-XX:ConcGCThreads'.
> 5. If there are lot of 'to-space exhausted' or 'to-space overflow' GC events, then increase the -XX:G1ReservePercent. The default is 10% of the Java heap. Note: G1 GC caps this value at 50%.

所以我们尝试调整各种参数试图解决问题：

```-XX:InitiatingHeapOccupancyPercent=45```  设置触发标记周期的 Java 堆占用率阈值，默认值是 45，之前不知道为什么调整到了 70，设置到 45 之后确实能更快的进行 old gc，但是那个很长时间的 young gc 仍然存在
```-XX:G1ReservePercent=20```  设置作为空闲空间的预留内存百分比，以降低目标空间溢出的风险，因为报的是 to-space exhausted，所以优先想到修改这个参数，网上也是这样推荐的，然而并没有什么卵用
```-XX:ParallelGCThreads=5```  如果逻辑处理器不止8个，则将n的值设置为逻辑处理器数的5/8左右，这是一个 magic number，据说可以解决很多问题，但是没有解决我们的问题
```-XX:G1HeapRegionSize=16M```  G1 区域的大小，值是 2 的幂，范围是 1MB 到 32MB 之间，目标是根据最小的 Java 堆大小划分出约 2048 个区域，默认值为 8M，设置成 16M 之后 8M 以上的对象才会被认为是大对象，可以降低大对象出现的频率，并且减少 old gen 的碎片化
```-XX:MetaspaceSize=256m```  设置这个参数也不是为了解决这次的问题，主要是看其他文章的时候发现这个也应该设置一下
```-XX:MaxTenuringThreshold=4```  为了让对象更快的从 young gen 进入 old gen，从而减轻 young gen 的压力，然而问题还是存在


调整了各种参数最终都没有解决问题，说实话内心是有点小崩溃的。这个时候恰好又看了下最新的 GC 日志，却这个时候却发现很多现象变得不太一样了。之前发生 lg young gc 的时候 FreePhysicalMemory 并没有下降，所以怀疑是 ```to-space exhausted``` 之后导致所有的都进了老年代，而其中并没有大对象，所以一直在调整 GC 参数，但是现在这个情况看下来还是可能有大对象的，然后再结合最新的 GC 日志推测确实是在哪个时间产生了大对象，后来发现是在某些场景下会把整个图片的 base64 都打印下来了，而有些照片的大小本身就 5M 了，打印日志的时候随便坐下拼接就超过 8M 了。

### 总结

虽然并没有真正通过 GC 调优解决这次的问题，但是在过程中还是对各个参数有了更深入的理解。而且```-XX:G1HeapRegionSize=16M```这个参数后来还帮助我们解决了另外一个服务 OOM  的问题。

